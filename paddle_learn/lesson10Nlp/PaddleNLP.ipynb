{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddleNLP\n",
    "\n",
    "介绍完计算机视觉的开发套件PaddleSeg后，让我们继续了解一个自然语言处理方面的开发套件PaddleNLP。PaddleNLP是基于Paddle框架开发的自然语言处理 (NLP) 开源项目，项目中包含工具、算法、模型和数据多种资源。PaddleNLP通过丰富的模型库、简洁易用的API，提供飞桨2.0的最佳实践并加速NLP领域应用产业落地效率。\n",
    "\n",
    "GitHub链接：https://github.com/PaddlePaddle/PaddleNLP\n",
    "\n",
    "丰富的模型库：涵盖了NLP主流应用相关的前沿模型，包括中文词向量、预训练模型、词法分析、文本分类、文本匹配、文本生成、机器翻译、通用对话、问答系统等。\n",
    "\n",
    "简洁易用的全流程API：深度兼容飞桨2.0的高层API体系，提供更多可复用的文本建模模块，可大幅度减少数据处理、组网、训练环节的代码开发，提高开发效率。不仅可以通过全流程API高效搭建自研的网络，还可以在现有模型库上便捷的二次研发。\n",
    "\n",
    "高性能分布式训练：通过高度优化的Transformer网络实现，结合混合精度与Fleet分布式训练API，可充分利用GPU集群资源，高效完成预训练模型的分布式训练。飞桨分布式技术已经支持千亿参数的NLP大模型训练，处于业界领先。\n",
    "\n",
    "如上图所示，PaddleNLP在数据加载、数据处理、模型组网和模型评估方面都提供了丰富的API。以组网单元为例，即提供了常用的通用网络结构如LSTM和GRU，也提供了一些领域专用的网络如CRF。更重要的是，很多网络结构还提供了预训练模型，如基于大量中文语料数据训练的Bert和Ernie模型。使用这些预训练模型可以通过一行的API调用来完成。\n",
    "\n",
    "在第五章自然语言处理的部分，我们已经使用飞桨框架搭建了LSTM模型来解决情感分类的问题。下面，我们将展示如何基于PaddleNLP中的预训练模型ERINE来实现另外一个文本分类的任务：对新闻标题进行分类。\n",
    "\n",
    "\n",
    "# 基于ERNIE模型的新闻标题分类\n",
    "\n",
    "**文本分类**是指人们使用计算机将文本数据进行自动化归类的任务，是自然语言处理（NLP）中的一项重要任务。在大数据、人工智能技术火热发展的今天，文本分类技术有着广泛的应用场景。比如，将文本分类应用在电商商品的评价方面，可以识别消费者对商品的态度，同时也可以从消费者的评论中提取很多有待改善的意见（第五章的情感分类案例）；将文本分类应用在广告过滤、假大空新闻识别、反黄识别等等方面，可以帮助新闻平台提取更多有价值的新闻，同时用户也能得到更多有价值的推荐新闻；将文本分类应用在垃圾邮件识别上，可以防止用户被过多垃圾信息打扰，避免错过重要邮件。\n",
    "\n",
    "本案例将基于PaddleNLP中的预训练模型**ERNIE**对**THUCNews**新闻标题数据进行文本分类，本实验一方面会带你学习ERNIE的原理，一方面将带你体验如何基于ERNIE 微调(fune-tuning)文本分类任务。不过在学习本案例之前，建议读者先了解下Transformer模型的概念。如果暂时没有精力，就可以将它当作一种比LSTM更加复杂的、适合处理序列数据的模型。\n",
    "\n",
    "本案例的模型实现方案如 **图2** 所示， 模型的输入是新闻标题的文本，模型的输出就是新闻标题的类别。在建模过程中，对于输入的新闻标题文本，首先需要进行数据处理生成规整的文本序列数据，包括语句分词、将词转换为id，过长文本截断、过短文本填充等等操作；然后使用预训练模型ERNIE对文本序列进行编码，获得文本的语义向量表示；最后经过全连接层和softmax处理得到文本属于各个新闻类别的概率。方案中不仅会使用ERNIE预训练模型，还会使用大量PaddleNLP的API，更便捷的完成数据处理和模型评估等工作。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/83d27e18211b4537943dae16af72161238eb0f49d66e473ca70685f830101acb\"></center>\n",
    "<center><br>图2 基于ERNIE文本任务的实验流程</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE的模型理论\n",
    "\n",
    "本实验将默认同学们已经了解BERT或者Transformer模型内容，如有需要，请参考[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)。在正式地开始实验之前，我们还是先来讨论一下ERNIE到底是什么？它有哪些亮点？为什么它有这么强悍的性能，在NLP领域取得多项突破？\n",
    "\n",
    "## ERNIE是什么?\n",
    "[ERNIE](https://arxiv.org/pdf/1904.09223.pdf)是百度发布一个[预训练模型]()，它通过引入三种级别的Knowledge Masking帮助模型学习语言知识，在多项任务上超越了BERT。在模型结构方面，它采用了Transformer的Encoder部分作为模型主干进行训练，如 **图3** (图片来自网络)所示。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/35c0a1190395471d9e4b6a2e536d34efb4dcd463cef443ad8bcaaf90a943d615\" width=50%></center>\n",
    "<center><br>图3 Transformer的Encoder部分</br></center>\n",
    "<br></br>\n",
    "\n",
    "关于ERNIE网络结构(Transformer Encoder)的工作原理，这里不再展开讨论。接下来，我们将聚焦在ERNIE本身的主要改进点进行讨论，即上边提到的三种Knowledge Masking策略。这三种策略都是应用在ERNIE预训练过程中的预训练任务，期望通过这三种级别的任务帮助ERNIE学到更多的语言知识。\n",
    "\n",
    "几个概念的简单说明：\n",
    "\n",
    "1. Seq2Seq模型: sequence to sequence模型是一类End-to-End的算法框架，也就是从序列到序列的转换模型框架，应用在机器翻译，自动应答等场景。Seq2Seq一般是通过Encoder-Decoder（编码-解码）框架实现，Encoder和Decoder部分可以是任意的文字，语音，图像，视频数据，模型可以采用CNN、RNN、LSTM、GRU、BLSTM等等。可见，Encoder-Decoder是一种模型框架或者说设计理念，我们可以据此设计出各种各样的应用算法。很多时候，我们也会将模型分为encoder和decoder两个结构分开使用，其中encoder产生的向量包括了整个输入Sequence的抽象。\n",
    "\n",
    "2. Attention机制（注意力）：注意力机制可以利用人类的认知机制直观解释。例如，我们的视觉系统倾向于关注图像中辅助判断的部分信息，并忽略掉不相关的信息。同样，在自然语言处理的问题中，输入的某些部分可能会比其他部分对决策更有帮助。例如，在翻译和总结任务中，输入序列中只有某些单词可能与预测下一个单词相关。同样，在image-caption问题中，输入图像中只有某些区域可能与生成caption的下一个单词更相关。注意力模型通过允许模型动态地关注有助于执行手头任务的输入的某些部分，而不是泛泛的关注全部信息。如果在Encoder-Decoder框架上实现Attenetion机制，相当于Encoder产生的向量并不是全部作用于每个输出，而是根据当前输出与向量的每个部分计算匹配权重（相当于是否需要投注意力的衡量），得到一个加权后的向量。这个加权后的向量相当于有选择的挑选了部分关注点。\n",
    "\n",
    "3. Transfomer模型：很多NLP的语义学习问题涉及到大量的训练数据，而RNN类的模型内部存在计算依赖，无法高效的并行化训练。使用Self-attenion的方法，将RNN变成每个输入与其他输入部分计算匹配度来决定注意力权重的方式，使得模型引入了Attention机制的同时也具备了并行化计算的能力。以这种Self-attention结构为核心，设计Encoder-Decoder的结构形成Transformer模型。BERT和ERNIE均是将Transformer的Encoder部分结构单独取出，用多个的非标记语料（转成标记数据，如填空/判断句子连续性等）的任务训练，并将得到的Encoder向量作为词汇的基础语义表示用于多种NLP任务（如阅读理解）的模型。\n",
    "\n",
    "\n",
    "## 2.2 Knowledge Masking Task \n",
    "\n",
    "训练语料中蕴含着大量的语言知识，例如词法，句法，语义信息，如何让模型有效地学习这些复杂的语言知识是一件有挑战的事情。BERT使用了MLM（masked language-model）和NSP（Next Sentence Prediction）两个预训练任务来进行训练，这两个任务可能并不能让BERT学到那么多复杂的语言知识，特别是后来多个研究人士提到NSP任务是比较简单的任务，它实际的作用不大。\n",
    "\n",
    "----\n",
    "**说明：**\n",
    "\n",
    "masked language-model（MLM）是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。\n",
    "Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。\n",
    "这两种任务的训练样本都可以根据大量语料自动生成，而不用人工标注，这也是这些任务被用于训练语义表示大模型的原因。\n",
    "\n",
    "----\n",
    "\n",
    "考虑到这一点，ERNIE提出了Knowledge Masking的策略，其包含三个级别：ERNIE将Knowledge分成了三个类别：`token级别(Basic-Level)`、`短语级别(Phrase-Level)` 和 `实体级别(Entity-Level)`。通过对这三个级别的对象进行Masking，提高模型对字词、短语的知识理解。\n",
    "\n",
    "**图4**展示了这三个级别的Masking策略和BERT Masking的对比，显然，Basic-Level Masking 同BERT的Masking一样，随机地对某些单词(如 written)进行Masking，在预训练过程中，让模型去预测这些被Mask后的单词；Phrase-Level Masking 是对语句中的短语进行masking，如 a series of；Entity-Level Masking是对语句中的实体词进行Masking，如人名 J. K. Rowling。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/093f3ff3205d43bf9521179e3db78dc1f427474122ac4dc59ab0dc1263396f75\" width=70%></center>\n",
    "<center><br>图4 ERNIE和BERT的Masking策略对比</br></center>\n",
    "<br></br>\n",
    "\n",
    "除了上边的Knowledge Masking外，ERNIE还采用多个**异源语料**帮助模型训练，例如对话数据，新闻数据，百科数据等等。通过这些改进以保证模型在字词、语句和语义方面更深入地学习到语言知识。当ERINE通过这些预训练任务学习之后，就会变成一个更懂语言知识的预训练模型，接下来，就可以应用ERINE在不同的**下游任务**进行微调，提高下游任务的效果。例如，用于文本分类任务：新闻标题的主题分类。\n",
    "\n",
    "> **异源语料** ：来自不同源头的数据，比如百度贴吧，百度新闻，维基百科等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于ERINE详细实现\n",
    "\n",
    "THUCNews新闻文本分类实验流程如 **图5** 所示，包含如下6个步骤：\n",
    "1. **数据处理**：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取；\n",
    "2. **模型构建**：设计新闻主题分类模型，判断标题所属的主题类别；\n",
    "3. **训练配置**：实例化模型，选择模型计算资源（CPU或者GPU），指定模型迭代的优化算法；\n",
    "4. **模型训练与评估**：执行多轮训练不断调整参数，以达到较好的效果；\n",
    "5. **模型保存**：将模型参数保存到指定位置，便于后续推理或继续训练使用；\n",
    "6. **模型预测**：对训练好的模型进行推理测试；\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c57aeb6152354f63b3f7f765fcefbbabaa75a874ec284129a5618fdbff001427\"></center>\n",
    "<center><br>图5 THUCNews文本分类的实验流程</br></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "## 数据处理\n",
    "首先，将基于paddleNLP套件中的ERNIE模型进行微调文本分类任务，因此在数据处理阶段，需要将数据按照paddleNLP规定的格式进行处理。但总体上数据处理的步骤基本是：读取数据至内存、分词并转换数据形式、构造DataLoader以组装数据为mini-batch形式，以便模型处理。虽然流程较长，但绝大多数自然语言处理任务均会如此处理，有过一次的经验就不陌生了。同时，paddleNLP对数据处理过程进行了高级API的封装，使得整个数据构造过程变得简洁了。\n",
    "\n",
    "### 数据集介绍\n",
    "THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档，均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。\n",
    "\n",
    "本案例使用的数据集是从[THUCNews新闻数据](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews)中根据新闻类别按照一定的比例提取了新闻标题，处理后的数据可以[点此下载](data/THUCNews_title.tar.gz)，其中训练集数据约27.1w，测试集约6.7w条。另外，我们已经根据新闻类别整理出了一份记录标签的词表 `label_dict.txt`。\n",
    "\n",
    "### 数据读取\n",
    "这里我们将构造一个数据集类THUCDataSet类，该类将完成数据读取，和数据获取的功能，即将训练集或者测试集通过该类进行加载，这会让处理数据的代码更加简洁、优雅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.io import Dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "\n",
    "class THUCDataSet(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        # 加载标签词典\n",
    "        self.label2id = self._load_label_dict(label_path)\n",
    "        # 加载数据集\n",
    "        self.data = self._load_data(data_path)\n",
    "\n",
    "        self.label_list = list(self.label2id.keys())\n",
    "    \n",
    "    # 加载数据集\n",
    "    def _load_data(self, data_path):\n",
    "        data_set = []\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                label, text = line.strip().split(\"\\t\", maxsplit=1)\n",
    "                example = {\"text\":text, \"label\": self.label2id[label]}\n",
    "                data_set.append(example)\n",
    "        return data_set\n",
    "    \n",
    "    # 加载标签词典\n",
    "    def _load_label_dict(self, label_path):\n",
    "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip().split() for line in f.readlines()]\n",
    "            lines = [(line[0], int(line[1])) for line in lines]\n",
    "            label_dict = dict(lines)\n",
    "        return label_dict\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  转换数据格式\n",
    "上边完成了数据集类THUCDataSet的定义，当我们加载数据后，通过索引获取其中的某条数据时，数据的格式如**图6**所示。每条数据都被封装为了一个字典，每条数据包含文本内容和对应的标签label。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/294208437e87483291b770d6b761e34917c2829a2fb34faca1b3b6b32e12f320\"></center>\n",
    "<center><br>图6 THUCDataSet中的一个样本</br></center>\n",
    "<br></br>\n",
    "\n",
    "但是ERNIE模型希望的语料数据输入格式同BERT，如**图7**所示，只有这样ERNIE才更方便将新的语料样本用于多种类型训练任务,如masked language-model、Next Sentence Prediction以及本案例需要完成的文本分类任务等。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5397d394d10c4a208b50dc3310a0d8d8b17b54c8730b42ff98bef50787f97794\"></center>\n",
    "<center><br>图7 ERNIE的数据输入格式</br></center>\n",
    "<br></br>\n",
    "\n",
    "因此，需要将以上生成数据转换成这样的输入格式，输入包括token ids 和 segment ids (通常也被称为token type ids)，position ids无需自己生成，模型内部可以自动生成， 转换之后如**图8**所示。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8fe5a94ead2d43bbb23f061fd41dc07e7ae4c02fe03b4fc6a8a70a361a7ed3ea\"></center>\n",
    "<center><br>图8 ERNIE数据输入样例</br></center>\n",
    "<br></br>\n",
    "\n",
    "这里在转换数据格式的时候，可以调用paddleNLP封装好的tokenizer，它可以帮助我们一行代码将原始数据转换成模型输入的格式。最后数据转换之后，如果有label，也需要返回做一个完整的样本。相关的代码如下，example[\"text\"]指定样本的文本，tokenizer指定我们期望使用的转换器。例如ERINE和BERT这样常用的模型，PaddleNLP均提供了线程的转换器。最后，通过转换结果encoded_inputs变量的不同字段，即可以得到不同的输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "\n",
    "    encoded_inputs = tokenizer(text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  构建DataLoader\n",
    "最后，我们需要构建一个DataLoader，帮我们将数据组装成规整的mini-batch的形式，以便传入模型进行处理。\n",
    "\n",
    "整个处理流程是这样的：首先使用convert_example处理数据为期望的格式，然后将调用[paddle.io.DataLoader](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/fluid/reader/DataLoader_cn.html)来构建DataLoader，在这个DataLoader生成mini-batch数据的过程中，会通过使用batchify_fn函数进行同一文本序列长度，处理lable等操作，以保证返回的数据适合输入模型中。\n",
    "\n",
    "相关代码如下，这里有两个比较重要的操作：\n",
    "1. [dataset.map(trans_fn)](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/datasets/dataset.py)：它会使用trans_fn( 即convert_example函数)处理每个样本成我们我们期待的数据格式。\n",
    "2. batchify_fn：在DataLoader提取出mini-batch数据后，这些数据是长短不一的，所以我们借助batchify_fn这个操作去规整化mini-batch数据为模型期望的样式，我们可以看到它的处理函数fn中包含3个操作，前两个是padding操作分别对应这token ids和segment ids，最后一个是stack操作处理的是label数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-02-15 20:27:25,452] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n",
      "\u001b[32m[2024-02-15 20:27:25,468] [    INFO]\u001b[0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2024-02-15 20:27:25,469] [    INFO]\u001b[0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    # trans_fn对应前边的covert_example函数，使用该函数处理每个样本为期望的格式\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # 调用paddle.io.DataLoader来构建DataLoader\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "MODEL_NAME = \"ernie-1.0\"\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),     # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id), # segment\n",
    "    Stack(dtype=\"int64\")\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在将数据组成批次的函数batchify_fn中使用了几个便捷的小工具：Stack, Pad, Tuple。读者可通过以下小示例清晰理解这几个函数的作用，分别用于将多个样本打包成一个批次、打包的过程中顺便做样本长度的补齐、特征集合与标签集合的映射三项操作。\n",
    "\n",
    "```\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = [3, 4, 5, 6]\n",
    "c = [5, 6, 7, 8]\n",
    "result = Stack()([a, b, c])\n",
    "print(\"Stacked Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = [5, 6, 7]\n",
    "c = [8, 9]\n",
    "result = Pad(pad_val=0)([a, b, c])\n",
    "print(\"Padded Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "data = [\n",
    "        [[1, 2, 3, 4], [1]],\n",
    "        [[5, 6, 7], [0]],\n",
    "        [[8, 9], [1]],\n",
    "       ]\n",
    "batchify_fn = Tuple(Pad(pad_val=0), Stack())\n",
    "ids, labels = batchify_fn(data)\n",
    "print(\"ids: \\n\", ids)\n",
    "print()\n",
    "print(\"labels: \\n\", labels)\n",
    "print()\n",
    "```\n",
    "\n",
    "输出结果：\n",
    "```\n",
    "Stacked Data: \n",
    " [[1 2 3 4]\n",
    " [3 4 5 6]\n",
    " [5 6 7 8]]\n",
    "\n",
    "Padded Data: \n",
    " [[1 2 3 4]\n",
    " [5 6 7 0]\n",
    " [8 9 0 0]]\n",
    "\n",
    "ids: \n",
    " [[1 2 3 4]\n",
    " [5 6 7 0]\n",
    " [8 9 0 0]]\n",
    "\n",
    "labels: \n",
    " [[1]\n",
    " [0]\n",
    " [1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建\n",
    "\n",
    "这里我们通过使用paddleNLP直接加载预训练好的ERNIE模型，在使用ERNIE对文本序列编码之后，如**图6**所示，在每个token的是指都可以获得一个编码向量，同时还可以获得<CLS>和<SEP>两个token对应的向量，其中<CLS>对应的向量可以视为代表文本序列的语义向量。将这个语义向量传入线性层便可以获得一个该文本串对应的类别向量。 \n",
    "  \n",
    "我们不妨称后边接入的适用文本分类的网络为**微调网络**，网络的定义如下，可以看到在这个定义中，我们使用paddleNLP加载了预训练好的erinie模型，后边定义了用于文本分类的线性层。同时，程序中也设置了使用Dropout的网络优化策略，在每批样本训练的时候会隐藏一部分网络来增加训练结果的鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErnieForSequenceClassification(paddle.nn.Layer):\n",
    "    def __init__(self, MODEL_NAME, num_class=14, dropout=None):\n",
    "        super(ErnieForSequenceClassification, self).__init__()\n",
    "        # 加载预训练好的ernie，只需要指定一个名字就可以\n",
    "        self.ernie = paddlenlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(ropout if dropout is not None else self.ernie.config[\"hidden_dropout_prob\"])\n",
    "        self.classifier = nn.Linear(self.ernie.config[\"hidden_size\"], num_class)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.ernie(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练配置\n",
    "\n",
    "在训练之前，我们定义模型训练时用到的一些组件和资源，包括定义模型的实例化对象，选择模型训练和或评估时需要使用的计算资源（CPU或者GPU），指定模型训练迭代的优化算法。 其中，文本分类的训练时间较久，我们将默认使用GPU进行训练，通过调用 [paddle.device.get_device()](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/device/get_device_cn.html) 来查看当前实验环境是否有GPU可用，优先使用GPU进行训练。\n",
    "\n",
    "另外，本实验将使用 [paddle.optimizer.AdamW()](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/adamw/AdamW_cn.html) 算法进行模型迭代优化，学习率的变化采用LinearDecayWithWarmup的策略曲线。\n",
    "\n",
    "最后，我们可以使用paddleNLP一键式加载训练好的ERNIE模型，这里读者可以通\"ernie-1.0\"加载base版的ERNIE，也可以通过\"ernie-tiny\"加载瘦身版的ERNIE，\"ernie-tiny\"较\"ernie-1.0\"会有更快的训练和推理速度。\n",
    "\n",
    "训练配置的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-02-15 20:27:37,804] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/ernie-1.0/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2024-02-15 20:27:37,805] [    INFO]\u001b[0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-1.0/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2024-02-15 20:27:38,471] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "W0215 20:27:38.688881 1068403 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.2, Runtime API Version: 12.0\n",
      "W0215 20:27:38.690380 1068403 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\n",
      "\u001b[33m[2024-02-15 20:27:39,747] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-1.0 were not used when initializing ErnieModel: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2024-02-15 20:27:39,748] [    INFO]\u001b[0m - All the weights of ErnieModel were initialized from the model checkpoint at ernie-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieModel for predictions without further training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 超参设置\n",
    "n_epochs = 1\n",
    "batch_size = 128\n",
    "max_seq_length = 128\n",
    "n_classes=14\n",
    "dropout_rate = None\n",
    "\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "\n",
    "MODEL_NAME = \"ernie-1.0\"\n",
    "\n",
    "# 加载数据集，构造DataLoader\n",
    "train_set = THUCDataSet(\"./train.txt\", \"label_dict.txt\")\n",
    "test_set = THUCDataSet(\"./test.txt\", \"label_dict.txt\")\n",
    "\n",
    "label2id = train_set.label2id\n",
    "train_set = MapDataset(train_set)\n",
    "test_set = MapDataset(test_set)\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "train_data_loader = create_dataloader(train_set, mode=\"train\", batch_size=batch_size, batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "test_data_loader = create_dataloader(test_set, mode=\"test\", batch_size=batch_size, batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "\n",
    "# 检测是否可以使用GPU，如果可以优先使用GPU\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\n",
    "if use_gpu:\n",
    "    paddle.set_device('gpu:0')\n",
    "\n",
    "# 加载预训练模型ERNIE\n",
    "# 加载用于文本分类的fune-tuning网络\n",
    "model =  ErnieForSequenceClassification(MODEL_NAME, num_class=n_classes, dropout=dropout_rate)\n",
    "\n",
    "# 设置优化器\n",
    "num_training_steps = len(train_data_loader) * n_epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与评估\n",
    "上文已经实现了数据处理、模型加载和训练配置功能，接下来就可以开始模型的训练了。在训练过程中，每执行完一轮便使用测试集进行评估，验证模型训练的效果。在模型训练和评估的时候，我们使用了[paddle.metric.Accuracy](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/metric/metrics/Accuracy_cn.html)帮助我们统计新闻的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 3.57374, acc: 0.07969\n",
      "global step 20, epoch: 1, batch: 20, loss: 3.50632, acc: 0.06953\n",
      "global step 30, epoch: 1, batch: 30, loss: 3.24069, acc: 0.07604\n",
      "global step 40, epoch: 1, batch: 40, loss: 2.83660, acc: 0.08867\n",
      "global step 50, epoch: 1, batch: 50, loss: 2.63905, acc: 0.11375\n",
      "global step 60, epoch: 1, batch: 60, loss: 2.02150, acc: 0.14948\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.69241, acc: 0.19431\n",
      "global step 80, epoch: 1, batch: 80, loss: 1.30972, acc: 0.24629\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.85141, acc: 0.30234\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.57540, acc: 0.35086\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.54959, acc: 0.39396\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.56799, acc: 0.43079\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.54519, acc: 0.46334\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.42362, acc: 0.49113\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.48272, acc: 0.51589\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.43414, acc: 0.53696\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.38832, acc: 0.55657\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.37764, acc: 0.57413\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.37101, acc: 0.58956\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.38826, acc: 0.60348\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.23632, acc: 0.61708\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.29838, acc: 0.62933\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.33851, acc: 0.64056\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.30350, acc: 0.65130\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.39533, acc: 0.66056\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.41074, acc: 0.66878\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.34906, acc: 0.67688\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.36972, acc: 0.68435\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.43661, acc: 0.69135\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.20169, acc: 0.69786\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.48507, acc: 0.70370\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.34475, acc: 0.70950\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.33735, acc: 0.71546\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.28833, acc: 0.72068\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.21934, acc: 0.72554\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.29402, acc: 0.73049\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.40539, acc: 0.73503\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.32948, acc: 0.73982\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.36721, acc: 0.74411\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.32815, acc: 0.74783\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.39946, acc: 0.75166\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.28299, acc: 0.75532\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.36734, acc: 0.75843\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.30288, acc: 0.76175\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.48189, acc: 0.76450\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.24456, acc: 0.76746\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.26412, acc: 0.77021\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.28585, acc: 0.77303\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.29234, acc: 0.77553\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.23059, acc: 0.77834\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.33813, acc: 0.78087\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.25810, acc: 0.78316\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.20762, acc: 0.78557\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.32683, acc: 0.78777\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.33404, acc: 0.79000\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.34345, acc: 0.79226\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.14938, acc: 0.79452\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.27812, acc: 0.79652\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.46580, acc: 0.79841\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.14829, acc: 0.80029\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.22780, acc: 0.80231\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.30533, acc: 0.80382\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.39821, acc: 0.80539\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.17580, acc: 0.80713\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.17798, acc: 0.80891\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.32680, acc: 0.81045\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.31373, acc: 0.81192\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.33449, acc: 0.81356\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.24476, acc: 0.81484\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.32672, acc: 0.81609\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.21540, acc: 0.81730\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.29168, acc: 0.81867\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.21106, acc: 0.81988\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.27189, acc: 0.82127\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.25541, acc: 0.82261\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.24531, acc: 0.82356\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.36146, acc: 0.82455\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.19718, acc: 0.82581\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.32024, acc: 0.82702\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.24206, acc: 0.82815\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.13157, acc: 0.82933\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.21412, acc: 0.83033\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.28708, acc: 0.83130\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.27795, acc: 0.83225\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.17089, acc: 0.83322\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.29723, acc: 0.83418\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.30792, acc: 0.83523\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.24261, acc: 0.83615\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.20381, acc: 0.83717\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.32110, acc: 0.83804\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.29375, acc: 0.83888\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.22824, acc: 0.83978\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.25630, acc: 0.84053\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.34812, acc: 0.84126\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.19559, acc: 0.84211\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.37886, acc: 0.84288\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.18435, acc: 0.84364\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.35775, acc: 0.84422\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.17318, acc: 0.84509\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.24355, acc: 0.84561\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.27479, acc: 0.84632\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.22231, acc: 0.84701\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.22507, acc: 0.84758\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.30658, acc: 0.84828\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.22288, acc: 0.84892\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.15918, acc: 0.84957\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.26413, acc: 0.85031\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.35776, acc: 0.85107\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.19859, acc: 0.85177\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.31399, acc: 0.85239\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.22283, acc: 0.85296\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.20659, acc: 0.85350\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.20501, acc: 0.85414\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.15728, acc: 0.85476\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.20079, acc: 0.85546\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.21802, acc: 0.85614\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.23146, acc: 0.85680\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.16978, acc: 0.85741\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.28613, acc: 0.85789\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.27586, acc: 0.85840\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.20323, acc: 0.85906\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.15575, acc: 0.85963\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.34147, acc: 0.86007\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.25505, acc: 0.86060\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.17992, acc: 0.86106\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.23078, acc: 0.86157\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.30342, acc: 0.86208\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.22843, acc: 0.86254\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.09525, acc: 0.86309\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.20466, acc: 0.86355\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.26162, acc: 0.86400\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.31023, acc: 0.86438\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.37204, acc: 0.86486\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.24723, acc: 0.86539\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.23548, acc: 0.86587\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.19987, acc: 0.86626\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.34694, acc: 0.86664\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.23040, acc: 0.86706\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.15734, acc: 0.86741\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.33055, acc: 0.86779\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.23943, acc: 0.86813\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.42875, acc: 0.86849\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.29759, acc: 0.86889\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.23473, acc: 0.86927\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.21098, acc: 0.86961\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.23397, acc: 0.86995\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.22023, acc: 0.87041\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.21490, acc: 0.87072\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.28429, acc: 0.87107\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.21202, acc: 0.87144\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.27998, acc: 0.87169\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.18083, acc: 0.87209\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.34862, acc: 0.87245\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.19177, acc: 0.87279\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.27222, acc: 0.87313\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.20720, acc: 0.87353\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.18855, acc: 0.87394\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.27996, acc: 0.87425\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.17428, acc: 0.87462\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.20740, acc: 0.87491\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.26694, acc: 0.87516\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.35870, acc: 0.87545\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.27060, acc: 0.87577\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.23009, acc: 0.87609\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.12089, acc: 0.87647\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.32510, acc: 0.87670\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.32504, acc: 0.87698\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.26116, acc: 0.87726\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.26522, acc: 0.87757\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.13823, acc: 0.87791\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.16400, acc: 0.87822\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.24689, acc: 0.87850\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.18515, acc: 0.87875\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.30548, acc: 0.87904\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.18005, acc: 0.87934\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.12975, acc: 0.87967\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.10920, acc: 0.87994\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.24304, acc: 0.88025\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.11686, acc: 0.88049\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.27676, acc: 0.88071\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.27817, acc: 0.88093\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.28032, acc: 0.88119\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.17646, acc: 0.88148\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.20906, acc: 0.88177\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.14112, acc: 0.88210\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.28568, acc: 0.88238\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.21608, acc: 0.88264\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.23651, acc: 0.88293\n",
      "global step 1890, epoch: 1, batch: 1890, loss: 0.17297, acc: 0.88316\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.31832, acc: 0.88338\n",
      "global step 1910, epoch: 1, batch: 1910, loss: 0.11135, acc: 0.88363\n",
      "global step 1920, epoch: 1, batch: 1920, loss: 0.18194, acc: 0.88381\n",
      "global step 1930, epoch: 1, batch: 1930, loss: 0.16631, acc: 0.88409\n",
      "global step 1940, epoch: 1, batch: 1940, loss: 0.23248, acc: 0.88429\n",
      "global step 1950, epoch: 1, batch: 1950, loss: 0.28601, acc: 0.88450\n",
      "global step 1960, epoch: 1, batch: 1960, loss: 0.12154, acc: 0.88473\n",
      "global step 1970, epoch: 1, batch: 1970, loss: 0.17308, acc: 0.88499\n",
      "global step 1980, epoch: 1, batch: 1980, loss: 0.13995, acc: 0.88522\n",
      "global step 1990, epoch: 1, batch: 1990, loss: 0.14956, acc: 0.88543\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.29950, acc: 0.88561\n",
      "global step 2010, epoch: 1, batch: 2010, loss: 0.15224, acc: 0.88582\n",
      "global step 2020, epoch: 1, batch: 2020, loss: 0.32421, acc: 0.88599\n",
      "global step 2030, epoch: 1, batch: 2030, loss: 0.17143, acc: 0.88621\n",
      "global step 2040, epoch: 1, batch: 2040, loss: 0.24185, acc: 0.88637\n",
      "global step 2050, epoch: 1, batch: 2050, loss: 0.34724, acc: 0.88658\n",
      "global step 2060, epoch: 1, batch: 2060, loss: 0.16565, acc: 0.88684\n",
      "global step 2070, epoch: 1, batch: 2070, loss: 0.20635, acc: 0.88706\n",
      "global step 2080, epoch: 1, batch: 2080, loss: 0.24369, acc: 0.88726\n",
      "global step 2090, epoch: 1, batch: 2090, loss: 0.15274, acc: 0.88748\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.19618, acc: 0.88764\n",
      "global step 2110, epoch: 1, batch: 2110, loss: 0.21138, acc: 0.88783\n",
      "eval loss: 0.19557, accu: 0.93778\n"
     ]
    }
   ],
   "source": [
    "# 定义统计指标\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    # 每次使用测试集进行评估时，先重置掉之前的metric的累计数据，保证只是针对本次评估。\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        # 获取数据\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        # 执行前向计算\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        # 计算损失\n",
    "        loss = F.cross_entropy(input=logits, label=labels)\n",
    "        loss= paddle.mean(loss)\n",
    "        losses.append(loss.numpy())\n",
    "        # 统计准确率指标\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    metric.reset()\n",
    "\n",
    "def train(model):\n",
    "    global_step=0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            # 获取数据\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            # 模型前向计算\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            loss = F.cross_entropy(input=logits, label=labels)\n",
    "            loss = paddle.mean(loss)\n",
    "\n",
    "            # 统计指标\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "            \n",
    "            # 打印中间训练结果\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 :\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "            \n",
    "            # 参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        \n",
    "        # 模型评估\n",
    "        evaluate(model, metric, test_data_loader)\n",
    "            \n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型保存\n",
    "在模型训练完成后，需要将模型和优化器参数保存到磁盘，用于模型推理或继续训练。另外也可以将tokenizer保存下来以备后用，实现代码如下所示：  \n",
    "通过使用paddle.save API实现模型参数和优化器参数的保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-02-15 20:46:25,685] [    INFO]\u001b[0m - tokenizer config file saved in ./tokenizer/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2024-02-15 20:46:25,686] [    INFO]\u001b[0m - Special tokens file saved in ./tokenizer/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型保存的名称\n",
    "model_name = \"ernie_for_sequence_classification\"\n",
    "\n",
    "paddle.save(model.state_dict(), \"{}.pdparams\".format(model_name))\n",
    "paddle.save(optimizer.state_dict(), \"{}.optparams\".format(model_name))\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测\n",
    "下面我们将构造一个data集合，使用集合中的语句测试模型效果。读者可以自行修改下述代码中的数据样例（data = [{\"text\":\"白羊座今天的运势很好\"}]），查看模型对不同语句的主题分类。以便比较容易地看到模型训练的效果。注意，使用模型之前，我们依然要将原始文本数据做Token转化和批次样本切分的操作。然后使用训练好的模型计算新闻在各个类别上的概率分布，选择概率最大的那个ID，转换成类别标签输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['游戏']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def predict(data, id2label, batch_size=1):\n",
    "    examples = []\n",
    "    # 数据处理\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # 将数据按照batch_size进行切分\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    # 使用模型预测数据，并返回结果\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [id2label[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n",
    "\n",
    "data = [{\"text\":\"英雄联盟\"}]\n",
    "\n",
    "id2label = dict([(items[1], items[0]) for items in label2id.items()])\n",
    "results = predict(data, id2label)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本节内容总结\n",
    "\n",
    "在第五章中我们介绍过类似的文本分类任务：情感分类，但当时更多是讲述如何使用飞桨框架搭建模型。本案例则基于预训练模型ERNIE对THUCNews新闻标题进行了文本分类，更多是介绍如何在NLP领域应用飞桨提供的各种预训练模型资源。在本案例中，我们探讨了ERNIE自身的改进点，主要是Knowledge Masing策略保证模型更深入的学习语言知识。另外，我们探讨了使用paddleNLP套件如何转换数据格式，如何一键式加载预训练模型，以及如何使用预训练模型在下游任务上进行微调(fune-tuning)。需要注意，自然语言处理任务与计算机视觉任务不同，需要有较为复杂的样本数据处理流程，这个差别也体现在飞桨的开发套件上。CV开发套件的数据处理部分是统一的，不会根据模型的不同而调整；而NLP开发套件的数据处理部分则与具体模型绑定，不同的模型会有自己的数据格式要求。\n",
    "\n",
    "最后，在本案例主要使用[ERNIE 1.0](https://arxiv.org/pdf/1904.09223.pdf)模型，百度后来发布了[ERNIE 2.0](https://arxiv.org/pdf/1907.12412.pdf)，它引入了更多了预训练任务学习，并且提出了一种有效的持续多任务式地学习方式保证ERNIE更有效的学习语言知识，感兴趣的读者可以查阅相关论文进行学习。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
