{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目地址：https://github.com/lanpa/tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soundfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# amplitude of sound should in [-1, 1]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     dummy_audio[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(freqs[n_iter \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(i) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(sample_rate))\n\u001b[0;32m---> 27\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyAudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext logged at step:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(n_iter), n_iter)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m resnet18\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[0;32m~/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/tensorboardX/writer.py:875\u001b[0m, in \u001b[0;36mSummaryWriter.add_audio\u001b[0;34m(self, tag, snd_tensor, global_step, sample_rate, walltime)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_caffe2_blob(snd_tensor):\n\u001b[1;32m    873\u001b[0m     snd_tensor \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(snd_tensor)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(\n\u001b[0;32m--> 875\u001b[0m     \u001b[43maudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnd_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m, global_step, walltime)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_comet_logger()\u001b[38;5;241m.\u001b[39mlog_audio(snd_tensor, sample_rate, tag, step\u001b[38;5;241m=\u001b[39mglobal_step)\n",
      "File \u001b[0;32m~/miniconda3/envs/paddle_learn/lib/python3.8/site-packages/tensorboardX/summary.py:413\u001b[0m, in \u001b[0;36maudio\u001b[0;34m(tag, tensor, sample_rate)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m  tensor: A 2-D float Tensor of shape `[frames, channels]` where `channels` is 1 or 2.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    The values should between [-1, 1]. We also accepts 1-D tensor.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m\n\u001b[1;32m    414\u001b[0m tensor \u001b[38;5;241m=\u001b[39m make_np(tensor)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(tensor)\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soundfile'"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(False)\n",
    "writer = SummaryWriter()\n",
    "sample_rate = 44100\n",
    "freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]\n",
    "\n",
    "for n_iter in range(100):\n",
    "\n",
    "    dummy_s1 = torch.rand(1)\n",
    "    dummy_s2 = torch.rand(1)\n",
    "    # data grouping by `slash`\n",
    "    writer.add_scalar('data/scalar1', dummy_s1[0], n_iter)\n",
    "    writer.add_scalar('data/scalar2', dummy_s2[0], n_iter)\n",
    "\n",
    "    writer.add_scalars('data/scalar_group', {'xsinx': n_iter * np.sin(n_iter),\n",
    "                                             'xcosx': n_iter * np.cos(n_iter),\n",
    "                                             'arctanx': np.arctan(n_iter)}, n_iter)\n",
    "\n",
    "    dummy_img = torch.rand(32, 3, 64, 64)  # output from network\n",
    "    if n_iter % 10 == 0:\n",
    "        x = vutils.make_grid(dummy_img, normalize=True, scale_each=True)\n",
    "        writer.add_image('Image', x, n_iter)\n",
    "\n",
    "        dummy_audio = torch.zeros(sample_rate * 2)\n",
    "        for i in range(x.size(0)):\n",
    "            # amplitude of sound should in [-1, 1]\n",
    "            dummy_audio[i] = np.cos(freqs[n_iter // 10] * np.pi * float(i) / float(sample_rate))\n",
    "        writer.add_audio('myAudio', dummy_audio, n_iter, sample_rate=sample_rate)\n",
    "\n",
    "        writer.add_text('Text', 'text logged at step:' + str(n_iter), n_iter)\n",
    "\n",
    "        for name, param in resnet18.named_parameters():\n",
    "            writer.add_histogram(name, param.clone().cpu().data.numpy(), n_iter)\n",
    "\n",
    "        # needs tensorboard 0.4RC or later\n",
    "        writer.add_pr_curve('xoxo', np.random.randint(2, size=100), np.random.rand(100), n_iter)\n",
    "\n",
    "dataset = datasets.MNIST('mnist', train=False, download=True)\n",
    "images = dataset.test_data[:100].float()\n",
    "label = dataset.test_labels[:100]\n",
    "\n",
    "features = images.view(100, 784)\n",
    "writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1))\n",
    "\n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
